{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TIL A collection of knowledge I learn throughout the day. Do not want to write a full blown blog on the topics but do want to record them for other folks who stumble upon them or myself, I have a terrible memory.","title":"TIL"},{"location":"#til","text":"A collection of knowledge I learn throughout the day. Do not want to write a full blown blog on the topics but do want to record them for other folks who stumble upon them or myself, I have a terrible memory.","title":"TIL"},{"location":"ansible/facts/","text":"Facts Facts are a bit of data on the server that configuration frameworks like puppet and ansible can read and take advantage of. The facts for ansible go in /etc/ansible/facts.d and can be a file that contains JSON or INI format or an executable script that returns JSON. The extension is .fact . These facts are then available via ansible <hostname> -m setup /etc/ansible/facts.d/preferences.fact [general] asdf=1 bar=2 ansible <hostname> -m setup -a \"filter=ansible_local\" \"ansible_local\": { \"preferences\": { \"general\": { \"asdf\" : \"1\", \"bar\" : \"2\" } } } Reference Ansible Facts.d","title":"Facts"},{"location":"ansible/facts/#facts","text":"Facts are a bit of data on the server that configuration frameworks like puppet and ansible can read and take advantage of. The facts for ansible go in /etc/ansible/facts.d and can be a file that contains JSON or INI format or an executable script that returns JSON. The extension is .fact . These facts are then available via ansible <hostname> -m setup /etc/ansible/facts.d/preferences.fact [general] asdf=1 bar=2 ansible <hostname> -m setup -a \"filter=ansible_local\" \"ansible_local\": { \"preferences\": { \"general\": { \"asdf\" : \"1\", \"bar\" : \"2\" } } }","title":"Facts"},{"location":"ansible/facts/#reference","text":"Ansible Facts.d","title":"Reference"},{"location":"ansible/handlers/","text":"Handlers Sometimes you want to trigger other tasks only if certain tasks have changed. This is where the notify attribute and handlers come into play. tasks: - template: src: \"README.md.j2\" dest: \"README.md\" notify: commit and push handlers: - name: commit and push shell: | git config user.name \"CircleCI\" git config user.email \"noreply@circleci.com\" git add README.md git commit -m \"Added newly generated README\" git push origin master","title":"Handlers"},{"location":"ansible/handlers/#handlers","text":"Sometimes you want to trigger other tasks only if certain tasks have changed. This is where the notify attribute and handlers come into play. tasks: - template: src: \"README.md.j2\" dest: \"README.md\" notify: commit and push handlers: - name: commit and push shell: | git config user.name \"CircleCI\" git config user.email \"noreply@circleci.com\" git add README.md git commit -m \"Added newly generated README\" git push origin master","title":"Handlers"},{"location":"asciidoc/color-text/","text":"Color Text You can color text in asciidoc by doing [<color>] #text# where <color> is one of the sixteen HTML color names .","title":"Color Text"},{"location":"asciidoc/color-text/#color-text","text":"You can color text in asciidoc by doing [<color>] #text# where <color> is one of the sixteen HTML color names .","title":"Color Text"},{"location":"authentication/saml-assertion-url/","text":"SAML Assersiont URL This is the URL that a SAML2 compliant IDP will redirect the request back too. There are two ways that this URL can be set and both work for specific mechanisms. Set in the IDP the ACS URL for an application. This is used when you go to an IDP and click on the application to sign in. Set on the SAML Request. This is for when you navigate to a page that requires SAML auth so the page sends you to the configured IDP for signin. Most times these much match but are used fro two completely separate signin mechanisms.","title":"SAML Assersiont URL"},{"location":"authentication/saml-assertion-url/#saml-assersiont-url","text":"This is the URL that a SAML2 compliant IDP will redirect the request back too. There are two ways that this URL can be set and both work for specific mechanisms. Set in the IDP the ACS URL for an application. This is used when you go to an IDP and click on the application to sign in. Set on the SAML Request. This is for when you navigate to a page that requires SAML auth so the page sends you to the configured IDP for signin. Most times these much match but are used fro two completely separate signin mechanisms.","title":"SAML Assersiont URL"},{"location":"authentication/saml-relaystate/","text":"Saml Relaystate The original meaning of RelayState is that the service provider can send some value to the identity provider together with the AuthnRequest and then get it back. The service provider can put whatever value it wants in the RelayState and the identity provider should just echo it back in the response. The main use case of RelayState is where the service provider redirects the user after login with the identity provider. Since there is a lot of URL hopping during SAML login, RelayState is perfect to provide context to the final leg on where to redirect the user too.","title":"Saml Relaystate"},{"location":"authentication/saml-relaystate/#saml-relaystate","text":"The original meaning of RelayState is that the service provider can send some value to the identity provider together with the AuthnRequest and then get it back. The service provider can put whatever value it wants in the RelayState and the identity provider should just echo it back in the response. The main use case of RelayState is where the service provider redirects the user after login with the identity provider. Since there is a lot of URL hopping during SAML login, RelayState is perfect to provide context to the final leg on where to redirect the user too.","title":"Saml Relaystate"},{"location":"aws/dynamic-references-cfn/","text":"Dynamic References CFN Cloudformation can resolve properties from certain resources via dynamic reference syntax. Example is \"{{resolve:secretsmanager:MyRDSSecret:SecretString:username}}\"","title":"Dynamic References CFN"},{"location":"aws/dynamic-references-cfn/#dynamic-references-cfn","text":"Cloudformation can resolve properties from certain resources via dynamic reference syntax. Example is \"{{resolve:secretsmanager:MyRDSSecret:SecretString:username}}\"","title":"Dynamic References CFN"},{"location":"aws/ec2-instancetype-latest-generation/","text":"EC2 InstanceType Latest Generation Just by using the latest instancetype for an intance family can reduce the AWS bill. Example: use the m5 family over the m4 family. jmoney8080/ec2-instancetype-generation is a script that returns a JSON payload that describes which ec2 instanes are at latest and which are not.","title":"EC2 InstanceType Latest Generation"},{"location":"aws/ec2-instancetype-latest-generation/#ec2-instancetype-latest-generation","text":"Just by using the latest instancetype for an intance family can reduce the AWS bill. Example: use the m5 family over the m4 family. jmoney8080/ec2-instancetype-generation is a script that returns a JSON payload that describes which ec2 instanes are at latest and which are not.","title":"EC2 InstanceType Latest Generation"},{"location":"aws/kinesis-kcl-checkpoint-table/","text":"Kinesis KCL Checkpoint Table Each row in the DynamoDB table represents a shard that is being processed by your application. The hash key for the table is leaseKey , which is the shard ID. In addition to the shard ID, each row also includes the following data: checkpoint: The most recent checkpoint sequence number for the shard. This value is unique across all shards in the stream. checkpointSubSequenceNumber: When using the Kinesis Producer Library's aggregation feature, this is an extension to checkpoint that tracks individual user records within the Kinesis record. leaseCounter: Used for lease versioning so that workers can detect that their lease has been taken by another worker. leaseKey: A unique identifier for a lease. Each lease is particular to a shard in the stream and is held by one worker at a time. leaseOwner: The worker that is holding this lease. ownerSwitchesSinceCheckpoint: How many times this lease has changed workers since the last time a checkpoint was written. parentShardId: Used to ensure that the parent shard is fully processed before processing starts on the child shards. This ensures that records are processed in the same order they were put into the stream.","title":"Kinesis KCL Checkpoint Table"},{"location":"aws/kinesis-kcl-checkpoint-table/#kinesis-kcl-checkpoint-table","text":"Each row in the DynamoDB table represents a shard that is being processed by your application. The hash key for the table is leaseKey , which is the shard ID. In addition to the shard ID, each row also includes the following data: checkpoint: The most recent checkpoint sequence number for the shard. This value is unique across all shards in the stream. checkpointSubSequenceNumber: When using the Kinesis Producer Library's aggregation feature, this is an extension to checkpoint that tracks individual user records within the Kinesis record. leaseCounter: Used for lease versioning so that workers can detect that their lease has been taken by another worker. leaseKey: A unique identifier for a lease. Each lease is particular to a shard in the stream and is held by one worker at a time. leaseOwner: The worker that is holding this lease. ownerSwitchesSinceCheckpoint: How many times this lease has changed workers since the last time a checkpoint was written. parentShardId: Used to ensure that the parent shard is fully processed before processing starts on the child shards. This ensures that records are processed in the same order they were put into the stream.","title":"Kinesis KCL Checkpoint Table"},{"location":"aws/lambda-container-image/","text":"Lambda Container Image AWS Lambda now supports container images. What this means is you can now deploy an AWS Lambda as a docker container. This seems to be a step forward from the \"bring your own runtimes\". new-for-aws-lambda-container-image-support","title":"Lambda Container Image"},{"location":"aws/lambda-container-image/#lambda-container-image","text":"AWS Lambda now supports container images. What this means is you can now deploy an AWS Lambda as a docker container. This seems to be a step forward from the \"bring your own runtimes\". new-for-aws-lambda-container-image-support","title":"Lambda Container Image"},{"location":"aws/lambda-docker-container/","text":"Lambda Docker Container docker run --rm -d \\ -e DOCKER_LAMBDA_STAY_OPEN=1 \\ -p 9001:9001 \\ -v $PWD:/var/task:ro,delegated \\ lambci/lambda:go1.x \\ main Run that in the code directory where the binary for the lambda you are executing exists. This will start up a local webserver in the same manner that AWS will run your lambda in. The lambda can now be invoked locally using aws lambda invoke --endpoint http://localhost:9001 --no-sign-request \\ --function-name myfunction --payload '{}' output.json or curl -d '{}' http://localhost:9001/2015-03-31/functions/myfunction/invocations The label of the docker image is lambda runtime that is to be used and the handler is what is set as the handler when createing a lambda. In the above example they are go1.x and main respectively.","title":"Lambda Docker Container"},{"location":"aws/lambda-docker-container/#lambda-docker-container","text":"docker run --rm -d \\ -e DOCKER_LAMBDA_STAY_OPEN=1 \\ -p 9001:9001 \\ -v $PWD:/var/task:ro,delegated \\ lambci/lambda:go1.x \\ main Run that in the code directory where the binary for the lambda you are executing exists. This will start up a local webserver in the same manner that AWS will run your lambda in. The lambda can now be invoked locally using aws lambda invoke --endpoint http://localhost:9001 --no-sign-request \\ --function-name myfunction --payload '{}' output.json or curl -d '{}' http://localhost:9001/2015-03-31/functions/myfunction/invocations The label of the docker image is lambda runtime that is to be used and the handler is what is set as the handler when createing a lambda. In the above example they are go1.x and main respectively.","title":"Lambda Docker Container"},{"location":"aws/list-all-instances-in-autoscaling-group/","text":"List All Instances in Autoscaling Group (export NAME=ASG_NAME; aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names ${NAME} | jq -r '.AutoScalingGroups[].Instances[].InstanceId')","title":"List All Instances in Autoscaling Group"},{"location":"aws/list-all-instances-in-autoscaling-group/#list-all-instances-in-autoscaling-group","text":"(export NAME=ASG_NAME; aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names ${NAME} | jq -r '.AutoScalingGroups[].Instances[].InstanceId')","title":"List All Instances in Autoscaling Group"},{"location":"aws/opensearch-best-practices/","text":"Opensearch Best Practices Opensearch is the AWS service that has been renamed from elasticsearch service which was their managed version of the OSS project elasticsearch. Fault Tolerance VPC Deployment Dedicated Masters Multi-AZ Deployment Cloudwatch Alarms Configuration Deployment Behavior","title":"Opensearch Best Practices"},{"location":"aws/opensearch-best-practices/#opensearch-best-practices","text":"Opensearch is the AWS service that has been renamed from elasticsearch service which was their managed version of the OSS project elasticsearch. Fault Tolerance VPC Deployment Dedicated Masters Multi-AZ Deployment Cloudwatch Alarms Configuration Deployment Behavior","title":"Opensearch Best Practices"},{"location":"aws/privatelink-alignment/","text":"PrivateLink Alignment It is a little known fact, but a rather annoying fact, that the availability zones for a PrivateLink must align between the provider and consumer. Example Provider is in availability zones: us-east-1a , us-east-1b , and us-east-1c Consumer is in availability zones: us-east-1b , us-east-1c , and us-east-1d The consumer IPs can ONLY be deployed to us-east-1b and us-east-1c as those are the AZs the provider is deployed too. This must be PHYSICAL availability zone. The availability zone us-east-1a is actually a virtual thing and maps to a physical availability zone. This mapping though can be different cross account. The describe-vpc-endpoints should tell you which availability zones you can deploy too as it handles the mapping into the account.","title":"PrivateLink Alignment"},{"location":"aws/privatelink-alignment/#privatelink-alignment","text":"It is a little known fact, but a rather annoying fact, that the availability zones for a PrivateLink must align between the provider and consumer.","title":"PrivateLink Alignment"},{"location":"aws/privatelink-alignment/#example","text":"Provider is in availability zones: us-east-1a , us-east-1b , and us-east-1c Consumer is in availability zones: us-east-1b , us-east-1c , and us-east-1d The consumer IPs can ONLY be deployed to us-east-1b and us-east-1c as those are the AZs the provider is deployed too. This must be PHYSICAL availability zone. The availability zone us-east-1a is actually a virtual thing and maps to a physical availability zone. This mapping though can be different cross account. The describe-vpc-endpoints should tell you which availability zones you can deploy too as it handles the mapping into the account.","title":"Example"},{"location":"aws/rds-replication-management/","text":"RDS Replication Management RDS provides an rds_replication role that is required for doing replication on RDS in postgres. This role does NOT have the REPLICATION priviledge required to muck with the WAL readers. So there must be some internal to RDS implementation that allows the replciation management. For instance, if you log into a postgres instance with a user with the rds_replication role you cannnot execute the pg_replication_slot_advance stored procedure to move the lsn for the slot around. Systems Administration The master user provided by RDS does not even have this role or SUPERUSER to grant this role. They have rds_superuser which does not have the ability to grant REPLICATION . Master User Accounts","title":"RDS Replication Management"},{"location":"aws/rds-replication-management/#rds-replication-management","text":"RDS provides an rds_replication role that is required for doing replication on RDS in postgres. This role does NOT have the REPLICATION priviledge required to muck with the WAL readers. So there must be some internal to RDS implementation that allows the replciation management. For instance, if you log into a postgres instance with a user with the rds_replication role you cannnot execute the pg_replication_slot_advance stored procedure to move the lsn for the slot around. Systems Administration The master user provided by RDS does not even have this role or SUPERUSER to grant this role. They have rds_superuser which does not have the ability to grant REPLICATION . Master User Accounts","title":"RDS Replication Management"},{"location":"aws/rds-update-configurations/","text":"RDS Update Configurations The user you get by default from RDS is an \"admin\" user but it is not THE admin user. To update configuration settings, like wal_sender_timeout , parameter groups must be used. If a query is executed in the console to update the configuration a permission denied will be thrown.","title":"RDS Update Configurations"},{"location":"aws/rds-update-configurations/#rds-update-configurations","text":"The user you get by default from RDS is an \"admin\" user but it is not THE admin user. To update configuration settings, like wal_sender_timeout , parameter groups must be used. If a query is executed in the console to update the configuration a permission denied will be thrown.","title":"RDS Update Configurations"},{"location":"aws/secrets-manager-decryt/","text":"Secrets Manager Decrypt Here is how to pretty print the json string stored in the SecretString field of the secrets manager aws secretsmanager get-secret-value --secret-id ${SECRET_ARN} | jq .SecretString | tr -d '\\\\' | sed 's/^\\\"\\(.*\\)\\\"$/\\1/' | jq .","title":"Secrets Manager Decrypt"},{"location":"aws/secrets-manager-decryt/#secrets-manager-decrypt","text":"Here is how to pretty print the json string stored in the SecretString field of the secrets manager aws secretsmanager get-secret-value --secret-id ${SECRET_ARN} | jq .SecretString | tr -d '\\\\' | sed 's/^\\\"\\(.*\\)\\\"$/\\1/' | jq .","title":"Secrets Manager Decrypt"},{"location":"aws/send-command-ssm/","text":"Send Command SSM Run command over instnces via multiple tags aws ssm send-command --document-name \"AWS-RunShellScript\" --parameters 'commands=[\"echo hello,world\"]' --targets Key=tag:tagA,Values=foo Key=tag:tagB,Values=Bar Key=tag:tabC,Values=Baz Notice the space between the \"Values\" and the \"Key\" in the --targets option.","title":"Send Command SSM"},{"location":"aws/send-command-ssm/#send-command-ssm","text":"","title":"Send Command SSM"},{"location":"aws/send-command-ssm/#run-command-over-instnces-via-multiple-tags","text":"aws ssm send-command --document-name \"AWS-RunShellScript\" --parameters 'commands=[\"echo hello,world\"]' --targets Key=tag:tagA,Values=foo Key=tag:tagB,Values=Bar Key=tag:tabC,Values=Baz Notice the space between the \"Values\" and the \"Key\" in the --targets option.","title":"Run command over instnces via multiple tags"},{"location":"aws/transit-gateway-cross-acount-sharing/","text":"Transit Gateway Cross Account Sharing In order to attach a VPC from Account 1 to a Transit Gateway in Account 2 the Transit Gateway in Acount 2 must be shared with Account 1 through resource sharing. This would require AWS Organizaions to be setup and enabled.","title":"Transit Gateway Cross Account Sharing"},{"location":"aws/transit-gateway-cross-acount-sharing/#transit-gateway-cross-account-sharing","text":"In order to attach a VPC from Account 1 to a Transit Gateway in Account 2 the Transit Gateway in Acount 2 must be shared with Account 1 through resource sharing. This would require AWS Organizaions to be setup and enabled.","title":"Transit Gateway Cross Account Sharing"},{"location":"aws/unused-ebs-volumes/","text":"Unused EBS Volumes Using the AWS cli it is possible to grab all the unused AWS volumes in an account/region pair. aws ec2 describe-volumes --filters Name=status,Values=available | jq . The above looks for all EBS volumes in the available state, but are not in-use or attached to an instance, and returns them all in JSON format. There are several states to look up. creating: Currently being created and initialized available: Available for use but not currently in use in-use: Currently being used or attached to an ec2 instance. deleting: Currently being deleted deleted: Has been deleted and AWS will reap shortly error: Some error has occured with the volume","title":"Unused EBS Volumes"},{"location":"aws/unused-ebs-volumes/#unused-ebs-volumes","text":"Using the AWS cli it is possible to grab all the unused AWS volumes in an account/region pair. aws ec2 describe-volumes --filters Name=status,Values=available | jq . The above looks for all EBS volumes in the available state, but are not in-use or attached to an instance, and returns them all in JSON format. There are several states to look up. creating: Currently being created and initialized available: Available for use but not currently in use in-use: Currently being used or attached to an ec2 instance. deleting: Currently being deleted deleted: Has been deleted and AWS will reap shortly error: Some error has occured with the volume","title":"Unused EBS Volumes"},{"location":"backstage/getting-started-with-backstage/","text":"Getting Started With Backstage Run the following command to create a backstage-app. nvm use 14.15.4 && npx @backstage/create-app","title":"Getting Started With Backstage"},{"location":"backstage/getting-started-with-backstage/#getting-started-with-backstage","text":"Run the following command to create a backstage-app. nvm use 14.15.4 && npx @backstage/create-app","title":"Getting Started With Backstage"},{"location":"clamav/cvd_update/","text":"Cvd Update The way to officially manage downloads of the clamav virus defintion database used to be clamdownloader.pl . There is a new python utility that is to replace that process in cvdupdate . The way these scripts work is they are run on a internal server. Freshclam is then configured to point at this internal server to pull the definitions from. So all the clamav instances do not request from the virus defintion origin directly but to this internal server. Cvdupdate is written to only pull changes when the virus definitions have actually changed.","title":"Cvd Update"},{"location":"clamav/cvd_update/#cvd-update","text":"The way to officially manage downloads of the clamav virus defintion database used to be clamdownloader.pl . There is a new python utility that is to replace that process in cvdupdate . The way these scripts work is they are run on a internal server. Freshclam is then configured to point at this internal server to pull the definitions from. So all the clamav instances do not request from the virus defintion origin directly but to this internal server. Cvdupdate is written to only pull changes when the virus definitions have actually changed.","title":"Cvd Update"},{"location":"docker/docker-compose-down/","text":"Docker Compose Down Ever seen the error docker-compose up Recreating example-search-platform_datastore_1 ... error ERROR: for example-search-platform_datastore_1 b'no such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000: No such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000' ERROR: for datastore b'no such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000: No such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000' ERROR: The image for the service you're trying to recreate has been removed. If you continue, volume data could be lost. Consider backing up your data before continuing. That is because you did not run docker-compose down but instead ran something like docker rmi -f $(docker images -q) to clean out the image cache. Docker compose has a tempoarary image cache it uses for orchestration and when you blow away the main docker image cache docker compose will get very confused and try to recreate the image.","title":"Docker Compose Down"},{"location":"docker/docker-compose-down/#docker-compose-down","text":"Ever seen the error docker-compose up Recreating example-search-platform_datastore_1 ... error ERROR: for example-search-platform_datastore_1 b'no such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000: No such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000' ERROR: for datastore b'no such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000: No such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000' ERROR: The image for the service you're trying to recreate has been removed. If you continue, volume data could be lost. Consider backing up your data before continuing. That is because you did not run docker-compose down but instead ran something like docker rmi -f $(docker images -q) to clean out the image cache. Docker compose has a tempoarary image cache it uses for orchestration and when you blow away the main docker image cache docker compose will get very confused and try to recreate the image.","title":"Docker Compose Down"},{"location":"golang/goreleaser/","text":"goreleaser When releasing binaries built in golang the currect defacto release tool is goreleaser . Personally, I love to have homebrew versions of my tooling but creating and managing taps can be annoying. Luckily, gorelaser has a nice hook into doing that. Just add the following snippet. brews: - github: owner: jmoney8080 name: homebrew-aws folder: Formula homepage: https://www.jmoney8080.dev description: Collection of scripts that analyze easily correctable items to save money in AWS. test: | system \"#{bin}/aws-cost-maintenance -h\"","title":"goreleaser"},{"location":"golang/goreleaser/#goreleaser","text":"When releasing binaries built in golang the currect defacto release tool is goreleaser . Personally, I love to have homebrew versions of my tooling but creating and managing taps can be annoying. Luckily, gorelaser has a nice hook into doing that. Just add the following snippet. brews: - github: owner: jmoney8080 name: homebrew-aws folder: Formula homepage: https://www.jmoney8080.dev description: Collection of scripts that analyze easily correctable items to save money in AWS. test: | system \"#{bin}/aws-cost-maintenance -h\"","title":"goreleaser"},{"location":"haproxy/defer-server-dns-lookups/","text":"Defer Server DNS lookups By default, HAProxy on start up will resolve all server DNS entries before the process is ready for traffic. To turn off this behavior add init-addr none to you server line. backend my_backend server my_server www.example.com:80 init-addr none","title":"Defer Server DNS lookups"},{"location":"haproxy/defer-server-dns-lookups/#defer-server-dns-lookups","text":"By default, HAProxy on start up will resolve all server DNS entries before the process is ready for traffic. To turn off this behavior add init-addr none to you server line. backend my_backend server my_server www.example.com:80 init-addr none","title":"Defer Server DNS lookups"},{"location":"haproxy/failover/","text":"Failover HAProxy has failover! backend mybackend server s1 10.0.0.101:80 check server s2 10.0.0.102:80 check server s3 10.0.0.103:80 check backup server s4 10.0.0.104:80 check backup In the above config s1 and s2 are the primary servers. They have the check keyword turned on which turns on healthchecking. If both fail then s3 is pulled into rotation. If s3 fails then it will pull s4 into rotation. Finally, if s4 fails it will return a http status code 503 SERVICE UNAVAILABLE . Reference HAProxy Failover","title":"Failover"},{"location":"haproxy/failover/#failover","text":"HAProxy has failover! backend mybackend server s1 10.0.0.101:80 check server s2 10.0.0.102:80 check server s3 10.0.0.103:80 check backup server s4 10.0.0.104:80 check backup In the above config s1 and s2 are the primary servers. They have the check keyword turned on which turns on healthchecking. If both fail then s3 is pulled into rotation. If s3 fails then it will pull s4 into rotation. Finally, if s4 fails it will return a http status code 503 SERVICE UNAVAILABLE .","title":"Failover"},{"location":"haproxy/failover/#reference","text":"HAProxy Failover","title":"Reference"},{"location":"haproxy/haproxy-and-lua/","text":"HAProxy and Lua HAProxy can be extended to do a lot more than just simple load balancing using the lua scripting language. I could go talk about just a few items but it would not do it justice. This shows 5 different concepts in how you can extend HAProxy to do more with the simple lua scripting language.","title":"HAProxy and Lua"},{"location":"haproxy/haproxy-and-lua/#haproxy-and-lua","text":"HAProxy can be extended to do a lot more than just simple load balancing using the lua scripting language. I could go talk about just a few items but it would not do it justice. This shows 5 different concepts in how you can extend HAProxy to do more with the simple lua scripting language.","title":"HAProxy and Lua"},{"location":"haproxy/service-discovery/","text":"Service Discovery HAProxy can actually leverage DNS SRV records and build a service discovery setup using these SRV records. First we need to configure some resolver options resolvers mydns nameserver dns1 <name server> accepted_payload_size 8192 # allow larger DNS payloads The key here is the accept_payload_size . We need to increase this as SRV records can be a lot and do not fit in the default payload size. Next we need to configure what is called server-template s. Server templates are a server line under the backend that expands to the maximum entries. If you have server-template 5 it will expand to 5 backend server lines based on the number of SRV records returned. backend webservers balance roundrobin server-template web 5 myservice.example.local:80 check resolvers mydns init-addr none","title":"Service Discovery"},{"location":"haproxy/service-discovery/#service-discovery","text":"HAProxy can actually leverage DNS SRV records and build a service discovery setup using these SRV records. First we need to configure some resolver options resolvers mydns nameserver dns1 <name server> accepted_payload_size 8192 # allow larger DNS payloads The key here is the accept_payload_size . We need to increase this as SRV records can be a lot and do not fit in the default payload size. Next we need to configure what is called server-template s. Server templates are a server line under the backend that expands to the maximum entries. If you have server-template 5 it will expand to 5 backend server lines based on the number of SRV records returned. backend webservers balance roundrobin server-template web 5 myservice.example.local:80 check resolvers mydns init-addr none","title":"Service Discovery"},{"location":"intellij/cannot-connect-localhost/","text":"Cannot Connect to Localhost This one is more of a bug TIL that has been annoying me lately. Somewhat recently IntelliJ started to fail connecting to localhost when syncing an maven based project. Caused by: java.rmi.ConnectException: Connection refused to host: 127.0.0.1; nested exception is: java.net.ConnectException: Connection refused (Connection refused) at java.rmi/sun.rmi.transport.tcp.TCPEndpoint.newSocket(TCPEndpoint.java:619) at java.rmi/sun.rmi.transport.tcp.TCPChannel.createConnection(TCPChannel.java:209) at java.rmi/sun.rmi.transport.tcp.TCPChannel.newConnection(TCPChannel.java:196) During troubleshooting, the caches were invalidated/restarted and the .idea directory was wiped and re-built but to no avail. Found an article that was the exact problem. Turns out, docker-for-desktop was adding a localhost line to the /etc/hosts file and for whatever reason DNS in intellij did not like that. ## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127.0.0.1 localhost 255.255.255.255 broadcasthost ::1 localhost # Added by Docker Desktop # To allow the same kube context to work on the host and the container: 127.0.0.1 kubernetes.docker.internal # End of section Removing the lines for docker-for-desktop fixed the issue.","title":"Cannot Connect to Localhost"},{"location":"intellij/cannot-connect-localhost/#cannot-connect-to-localhost","text":"This one is more of a bug TIL that has been annoying me lately. Somewhat recently IntelliJ started to fail connecting to localhost when syncing an maven based project. Caused by: java.rmi.ConnectException: Connection refused to host: 127.0.0.1; nested exception is: java.net.ConnectException: Connection refused (Connection refused) at java.rmi/sun.rmi.transport.tcp.TCPEndpoint.newSocket(TCPEndpoint.java:619) at java.rmi/sun.rmi.transport.tcp.TCPChannel.createConnection(TCPChannel.java:209) at java.rmi/sun.rmi.transport.tcp.TCPChannel.newConnection(TCPChannel.java:196) During troubleshooting, the caches were invalidated/restarted and the .idea directory was wiped and re-built but to no avail. Found an article that was the exact problem. Turns out, docker-for-desktop was adding a localhost line to the /etc/hosts file and for whatever reason DNS in intellij did not like that. ## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127.0.0.1 localhost 255.255.255.255 broadcasthost ::1 localhost # Added by Docker Desktop # To allow the same kube context to work on the host and the container: 127.0.0.1 kubernetes.docker.internal # End of section Removing the lines for docker-for-desktop fixed the issue.","title":"Cannot Connect to Localhost"},{"location":"java/hibernate-validator-optional/","text":"Hibernate Validator Optional Apparently, the correct way of validating a type within an optional is the following @QueryParam(\"origin\") Optional<@URL(protocol = \"http\") String> origin, instead of @QueryParam(\"origin\") @URL(protocol = \"http\") Optional<String> origin, When doing the latter the following exception is returned plain HV000030: No validator could be found for constraint 'org.hibernate.validator.constraints.URL' validating type 'java.util.Optional<java.lang.String>'. .","title":"Hibernate Validator Optional"},{"location":"java/hibernate-validator-optional/#hibernate-validator-optional","text":"Apparently, the correct way of validating a type within an optional is the following @QueryParam(\"origin\") Optional<@URL(protocol = \"http\") String> origin, instead of @QueryParam(\"origin\") @URL(protocol = \"http\") Optional<String> origin, When doing the latter the following exception is returned plain HV000030: No validator could be found for constraint 'org.hibernate.validator.constraints.URL' validating type 'java.util.Optional<java.lang.String>'. .","title":"Hibernate Validator Optional"},{"location":"javascript/regex/","text":"Regex The tl;dr - need to be careful with regex's in javascript and the g flag as this can cause some caching to happen when using regex. const userMatcher = /user=(.+?)(?:;\\s?|$)/g function getUser(cookie) { const matches = userMatcher.exec(cookie); if (!matches || matches.length < 2) { return ''; } return matches[1]; } let cookie, user; cookie = `other=110011; user=aaBBccDDeeFFgg; other=12345`; user = getUser(cookie); console.log({ user }); // should be 'aaBBccDDeeFFgg' cookie = `user=112233445566; other=12345`; user = getUser(cookie); console.log({ user }); // should be '112233445566' The Problem userMatcher has the g flag on, so the regex keeps track of the last match position and starts from there on a second call to exec. The Fix Removing the g flag did it for me. But if you can\u2019t, reset the regex by setting lastIndex=0 after each use.","title":"Regex"},{"location":"javascript/regex/#regex","text":"The tl;dr - need to be careful with regex's in javascript and the g flag as this can cause some caching to happen when using regex. const userMatcher = /user=(.+?)(?:;\\s?|$)/g function getUser(cookie) { const matches = userMatcher.exec(cookie); if (!matches || matches.length < 2) { return ''; } return matches[1]; } let cookie, user; cookie = `other=110011; user=aaBBccDDeeFFgg; other=12345`; user = getUser(cookie); console.log({ user }); // should be 'aaBBccDDeeFFgg' cookie = `user=112233445566; other=12345`; user = getUser(cookie); console.log({ user }); // should be '112233445566'","title":"Regex"},{"location":"javascript/regex/#the-problem","text":"userMatcher has the g flag on, so the regex keeps track of the last match position and starts from there on a second call to exec.","title":"The Problem"},{"location":"javascript/regex/#the-fix","text":"Removing the g flag did it for me. But if you can\u2019t, reset the regex by setting lastIndex=0 after each use.","title":"The Fix"},{"location":"jersey/container-request-filter-priorities/","text":"Container Request Filter Priorities A collection of built-in priority constants for the components that are supposed to be ordered based on their javax.annotation.Priority class-level annotation value when used or applied by the runtime. For example, filters and interceptors are grouped in chains for each of the message processing extension points: Pre, PreMatch, Post as well as ReadFrom and WriteTo. Each of these chains is sorted based on priorities which are represented as integer numbers. All chains, except Post, are sorted in ascending order; the lower the number the higher the priority. The Post filter chain is sorted in descending order to ensure that response filters are executed in reverse order. Components that belong to the same priority class (same integer value) are executed in an implementation-defined manner. By default, when the @Priority annotation is absent on a component, for which a priority should be applied, the USER priority value is used.","title":"Container Request Filter Priorities"},{"location":"jersey/container-request-filter-priorities/#container-request-filter-priorities","text":"A collection of built-in priority constants for the components that are supposed to be ordered based on their javax.annotation.Priority class-level annotation value when used or applied by the runtime. For example, filters and interceptors are grouped in chains for each of the message processing extension points: Pre, PreMatch, Post as well as ReadFrom and WriteTo. Each of these chains is sorted based on priorities which are represented as integer numbers. All chains, except Post, are sorted in ascending order; the lower the number the higher the priority. The Post filter chain is sorted in descending order to ensure that response filters are executed in reverse order. Components that belong to the same priority class (same integer value) are executed in an implementation-defined manner. By default, when the @Priority annotation is absent on a component, for which a priority should be applied, the USER priority value is used.","title":"Container Request Filter Priorities"},{"location":"openssl/base64/","text":"Base64 To base64(sha1(string)) where the padding is removed AND base64 encoding the raw bytes and not the hex string run echo -n foo | openssl dgst -binary -sha1 | openssl base64 -A | tr -d '='","title":"Base64"},{"location":"openssl/base64/#base64","text":"To base64(sha1(string)) where the padding is removed AND base64 encoding the raw bytes and not the hex string run echo -n foo | openssl dgst -binary -sha1 | openssl base64 -A | tr -d '='","title":"Base64"},{"location":"postgres/PGPASSWORD/","text":"PGPASSWORD To pass a password to psql without using the iteractive promp use the PGPASSWORD environment variable. PGPASSWORD=password psql --host localhost --port 5432 --username admin -d public -f script.sql","title":"PGPASSWORD"},{"location":"postgres/PGPASSWORD/#pgpassword","text":"To pass a password to psql without using the iteractive promp use the PGPASSWORD environment variable. PGPASSWORD=password psql --host localhost --port 5432 --username admin -d public -f script.sql","title":"PGPASSWORD"},{"location":"postgres/active-wal-readers-on-slot/","text":"Active WAL Readers on Slot SELECT psa.usename, psa.application_name, psa.client_addr, psa.datname, prs.slot_name, psa.state, psa.wait_event, prs.restart_lsn, prs.confirmed_flush_lsn FROM pg_replication_slots prs JOIN pg_stat_activity psa ON prs.active_pid = psa.pid WHERE prs.slot_name = 'kinesis';","title":"Active WAL Readers on Slot"},{"location":"postgres/active-wal-readers-on-slot/#active-wal-readers-on-slot","text":"SELECT psa.usename, psa.application_name, psa.client_addr, psa.datname, prs.slot_name, psa.state, psa.wait_event, prs.restart_lsn, prs.confirmed_flush_lsn FROM pg_replication_slots prs JOIN pg_stat_activity psa ON prs.active_pid = psa.pid WHERE prs.slot_name = 'kinesis';","title":"Active WAL Readers on Slot"},{"location":"postgres/advance-walreader/","text":"Advance a WAL Reader SELECT pg_replication_slot_advance('slot_name', '28/160E2250')","title":"Advance a WAL Reader"},{"location":"postgres/advance-walreader/#advance-a-wal-reader","text":"SELECT pg_replication_slot_advance('slot_name', '28/160E2250')","title":"Advance a WAL Reader"},{"location":"postgres/destructively-clear-out-replication-slot/","text":"Destructively Clear Out Replication Slot This is not to be done in a production environment. SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot eists SELECT pg_drop_replication_slot('slot_name'); -- Drop slot SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot does not exists SELECT pg_sleep(300); -- Wait for some specified amount of time for WAL files to clear SELECT pg_create_logical_replication_slot('slot_name', 'wal2json'); -- create new logical slot and hookup wal2json plugin SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot eists The above is useful for clearing out WAL files in RDS if the WAL reader fell behind and we just want to get the wal reader to the end of the WAL. Usually waiting 5 to 10 minutes is good enough to clear out all the obsolete WAL files. Running the query in Disk Space Queries should be a good gauge if the WAL files are deleted.","title":"Destructively Clear Out Replication Slot"},{"location":"postgres/destructively-clear-out-replication-slot/#destructively-clear-out-replication-slot","text":"This is not to be done in a production environment. SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot eists SELECT pg_drop_replication_slot('slot_name'); -- Drop slot SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot does not exists SELECT pg_sleep(300); -- Wait for some specified amount of time for WAL files to clear SELECT pg_create_logical_replication_slot('slot_name', 'wal2json'); -- create new logical slot and hookup wal2json plugin SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot eists The above is useful for clearing out WAL files in RDS if the WAL reader fell behind and we just want to get the wal reader to the end of the WAL. Usually waiting 5 to 10 minutes is good enough to clear out all the obsolete WAL files. Running the query in Disk Space Queries should be a good gauge if the WAL files are deleted.","title":"Destructively Clear Out Replication Slot"},{"location":"postgres/disk-space-queries/","text":"Disk Space Queries Here are a few queries to figure out what is consuming all the disk space in postgres WAL Files select count(*) \"#WAL files\", round(sum(size) / 1024.0/1024.0/1024.0,3) as \"Size (GB)\", min(modification) \"Oldest\", max(modification) \"Newest\" from pg_ls_waldir(); If the above query is showing substantial disk usage then run the following to determine what replication slot is behind select *, pg_current_wal_flush_lsn() from pg_replication_slots; Really Large Tables select nspname \"Schema\", relname \"Table\", reltuples \"#Rows Est\", pg_size_pretty(size) \"Disk Space\" from ( select n.nspname,c.relname,c.reltuples, pg_table_size(c.oid) size from pg_class c join pg_namespace n on n.oid = c.relnamespace where c.relkind = 'r' ) a order by size desc; The above will show how much disk is being consumed by tables.","title":"Disk Space Queries"},{"location":"postgres/disk-space-queries/#disk-space-queries","text":"Here are a few queries to figure out what is consuming all the disk space in postgres","title":"Disk Space Queries"},{"location":"postgres/disk-space-queries/#wal-files","text":"select count(*) \"#WAL files\", round(sum(size) / 1024.0/1024.0/1024.0,3) as \"Size (GB)\", min(modification) \"Oldest\", max(modification) \"Newest\" from pg_ls_waldir(); If the above query is showing substantial disk usage then run the following to determine what replication slot is behind select *, pg_current_wal_flush_lsn() from pg_replication_slots;","title":"WAL Files"},{"location":"postgres/disk-space-queries/#really-large-tables","text":"select nspname \"Schema\", relname \"Table\", reltuples \"#Rows Est\", pg_size_pretty(size) \"Disk Space\" from ( select n.nspname,c.relname,c.reltuples, pg_table_size(c.oid) size from pg_class c join pg_namespace n on n.oid = c.relnamespace where c.relkind = 'r' ) a order by size desc; The above will show how much disk is being consumed by tables.","title":"Really Large Tables"},{"location":"postgres/largest-row-by-size/","text":"Largest Row By Size Here is how to select the PK of the row with the largest row size. SELECT <pk>, pg_column_size(t.*) AS row_size FROM <table> AS t ORDER BY pg_column_size(t.*) DESC LIMIT 1; Output is something like plain pk | row_size --------+---------- 1 | 112766","title":"Largest Row By Size"},{"location":"postgres/largest-row-by-size/#largest-row-by-size","text":"Here is how to select the PK of the row with the largest row size. SELECT <pk>, pg_column_size(t.*) AS row_size FROM <table> AS t ORDER BY pg_column_size(t.*) DESC LIMIT 1; Output is something like plain pk | row_size --------+---------- 1 | 112766","title":"Largest Row By Size"},{"location":"postgres/null-character-text-fields/","text":"Null Character Text Fields Postgres does not support the NULL character being inserted into text fields. They need to be preprocessed before being inserted into the table or a trigger that strips the 0x00 character which filters the column before writing into the text field. Preprocess In Python it is simple to preprocess the text line to replace all NULL characters. writer = csv.DictWriter(f_out, fieldnames=fields) for line in f_in: writer.writerow(line.replace(b'0x00', b'')) # strip any null bytes sequence in the stream file and write the row Then a COPY \"table\" FROM STDIN CSV HEADER can be issued piping in the file written above to psql. Before Trigger The following function and trigger are examples of how to pre-process the column before storing into the table. CREATE OR REPLACE FUNCTION blob2text() RETURNS void AS $$ DECLARE i integer; BEGIN -- find 0x00 and replace with space i := position(E'\\\\000'::bytea in NEW.text_field); WHILE i > 0 LOOP NEW.text_field := set_byte(NEW.text_field, i-1, 20); i := position(E'\\\\000'::bytea in NEW.text_field); END LOOP NEW.text_field = encode(NEW.text_field, 'escape'); END; $$ LANGUAGE plpgsql; Then you would just need to create the trigger. CREATE TRIGGER pre_process_null_character BEFORE INSERT OR UPDATE ON table FOR EACH ROW EXECUTE PROCEDURE blob2text(); References SQL SYNTAX STRING UESCAPE Stackoverflow","title":"Null Character Text Fields"},{"location":"postgres/null-character-text-fields/#null-character-text-fields","text":"Postgres does not support the NULL character being inserted into text fields. They need to be preprocessed before being inserted into the table or a trigger that strips the 0x00 character which filters the column before writing into the text field.","title":"Null Character Text Fields"},{"location":"postgres/null-character-text-fields/#preprocess","text":"In Python it is simple to preprocess the text line to replace all NULL characters. writer = csv.DictWriter(f_out, fieldnames=fields) for line in f_in: writer.writerow(line.replace(b'0x00', b'')) # strip any null bytes sequence in the stream file and write the row Then a COPY \"table\" FROM STDIN CSV HEADER can be issued piping in the file written above to psql.","title":"Preprocess"},{"location":"postgres/null-character-text-fields/#before-trigger","text":"The following function and trigger are examples of how to pre-process the column before storing into the table. CREATE OR REPLACE FUNCTION blob2text() RETURNS void AS $$ DECLARE i integer; BEGIN -- find 0x00 and replace with space i := position(E'\\\\000'::bytea in NEW.text_field); WHILE i > 0 LOOP NEW.text_field := set_byte(NEW.text_field, i-1, 20); i := position(E'\\\\000'::bytea in NEW.text_field); END LOOP NEW.text_field = encode(NEW.text_field, 'escape'); END; $$ LANGUAGE plpgsql; Then you would just need to create the trigger. CREATE TRIGGER pre_process_null_character BEFORE INSERT OR UPDATE ON table FOR EACH ROW EXECUTE PROCEDURE blob2text();","title":"Before Trigger"},{"location":"postgres/null-character-text-fields/#references","text":"SQL SYNTAX STRING UESCAPE Stackoverflow","title":"References"},{"location":"postgres/postgres-connection-with-replication-api/","text":"Postgres Connection with Replication Api If the connection has the replication api enabled then normal queries are blocked on the WalSenderWaitForWAL wait_event.","title":"Postgres Connection with Replication Api"},{"location":"postgres/postgres-connection-with-replication-api/#postgres-connection-with-replication-api","text":"If the connection has the replication api enabled then normal queries are blocked on the WalSenderWaitForWAL wait_event.","title":"Postgres Connection with Replication Api"},{"location":"postgres/postgres-tls/","text":"Postgres TLS TThe protocol states you have to connect to it in plaintext first to ask postgres should it use TLS. If postgres says \u201cuse tls\u201d then it starts the tls handshake on the same port. If no, it keeps going plaintext on the same port. Basically, the same port for both plaintext/tls and postgres just tells the client what protocol to use. Example openssl command Requires openssl >= 1.1.1 (export SERVER=<server>; export PORT=<port> ; openssl s_client -starttls postgres -connect ${SERVER}:${PORT} -showcerts)","title":"Postgres TLS"},{"location":"postgres/postgres-tls/#postgres-tls","text":"TThe protocol states you have to connect to it in plaintext first to ask postgres should it use TLS. If postgres says \u201cuse tls\u201d then it starts the tls handshake on the same port. If no, it keeps going plaintext on the same port. Basically, the same port for both plaintext/tls and postgres just tells the client what protocol to use.","title":"Postgres TLS"},{"location":"postgres/postgres-tls/#example-openssl-command","text":"Requires openssl >= 1.1.1 (export SERVER=<server>; export PORT=<port> ; openssl s_client -starttls postgres -connect ${SERVER}:${PORT} -showcerts)","title":"Example openssl command"},{"location":"postgres/replication-lag-logical-slots/","text":"Replication Lag Logical Slots This query will calculate the number of bytes between the current LSN and the confirmed LSN reported by the slot. SELECT slot_name, confirmed_flush_lsn, pg_current_wal_lsn(), (pg_current_wal_lsn() - confirmed_flush_lsn) AS lsn_distance FROM pg_replication_slots;","title":"Replication Lag Logical Slots"},{"location":"postgres/replication-lag-logical-slots/#replication-lag-logical-slots","text":"This query will calculate the number of bytes between the current LSN and the confirmed LSN reported by the slot. SELECT slot_name, confirmed_flush_lsn, pg_current_wal_lsn(), (pg_current_wal_lsn() - confirmed_flush_lsn) AS lsn_distance FROM pg_replication_slots;","title":"Replication Lag Logical Slots"},{"location":"postgres/wal-senders-timeout/","text":"WAL Sender Timeout wal_sender_timeout (integer) Terminate replication connections that are inactive longer than the specified number of milliseconds. This is useful for the sending server to detect a standby crash or network outage. A value of zero disables the timeout mechanism. This parameter can only be set in the postgresql.conf file or on the server command line. The default value is 60 seconds. The following will show what the current value is for the wal_sender_timeout . SHOW wal_sender_timeout; The following will update the current value for wal_sender_timeout to 0 which disables the timeout. ALTER SYSTEM SET wal_sender_timeout TO 0;","title":"WAL Sender Timeout"},{"location":"postgres/wal-senders-timeout/#wal-sender-timeout","text":"wal_sender_timeout (integer) Terminate replication connections that are inactive longer than the specified number of milliseconds. This is useful for the sending server to detect a standby crash or network outage. A value of zero disables the timeout mechanism. This parameter can only be set in the postgresql.conf file or on the server command line. The default value is 60 seconds. The following will show what the current value is for the wal_sender_timeout . SHOW wal_sender_timeout; The following will update the current value for wal_sender_timeout to 0 which disables the timeout. ALTER SYSTEM SET wal_sender_timeout TO 0;","title":"WAL Sender Timeout"},{"location":"postgres/wal2json-nextlsn/","text":"Wal2Json Next LSN Wal2Json can be configured to emit the next lsn . This is particularly helpful with coupled with logging and your processor is stuck processing a particcular lsn. If the next lsn is logged during processing and the processor is stuck for whatever reasion the next lsn will tell you the next lsn to set in postgres such that the problematic transaction is skipped. To enable add -o include_lsn=true as config to wal2json. The default value is false as of wal2json 2.3.","title":"Wal2Json Next LSN"},{"location":"postgres/wal2json-nextlsn/#wal2json-next-lsn","text":"Wal2Json can be configured to emit the next lsn . This is particularly helpful with coupled with logging and your processor is stuck processing a particcular lsn. If the next lsn is logged during processing and the processor is stuck for whatever reasion the next lsn will tell you the next lsn to set in postgres such that the problematic transaction is skipped. To enable add -o include_lsn=true as config to wal2json. The default value is false as of wal2json 2.3.","title":"Wal2Json Next LSN"},{"location":"postgres/what-privileges-are-assigned-to-a-user/","text":"What privileges are assigned to a user SELECT grantee ,table_catalog ,table_schema ,table_name ,string_agg(privilege_type, ', ' ORDER BY privilege_type) AS privileges FROM information_schema.role_table_grants WHERE grantee != 'postgres' GROUP BY grantee, table_catalog, table_schema, table_name; The above query will list all users, who are not postgres, and their assigned privileges.","title":"What privileges are assigned to a user"},{"location":"postgres/what-privileges-are-assigned-to-a-user/#what-privileges-are-assigned-to-a-user","text":"SELECT grantee ,table_catalog ,table_schema ,table_name ,string_agg(privilege_type, ', ' ORDER BY privilege_type) AS privileges FROM information_schema.role_table_grants WHERE grantee != 'postgres' GROUP BY grantee, table_catalog, table_schema, table_name; The above query will list all users, who are not postgres, and their assigned privileges.","title":"What privileges are assigned to a user"},{"location":"postgres/what-user-owns-tables-in-a-schema/","text":"What user owns tables in a schema Ever wonder what users own tables in a schema? Well, this is how you do it in postgres select t.table_name, t.table_type, c.relname, c.relowner, u.usename from information_schema.tables t join pg_catalog.pg_class c on (t.table_name = c.relname) join pg_catalog.pg_user u on (c.relowner = u.usesysid) where t.table_schema='public'; In the query above replace public with whatever schema is of interest.","title":"What user owns tables in a schema"},{"location":"postgres/what-user-owns-tables-in-a-schema/#what-user-owns-tables-in-a-schema","text":"Ever wonder what users own tables in a schema? Well, this is how you do it in postgres select t.table_name, t.table_type, c.relname, c.relowner, u.usename from information_schema.tables t join pg_catalog.pg_class c on (t.table_name = c.relname) join pg_catalog.pg_user u on (c.relowner = u.usesysid) where t.table_schema='public'; In the query above replace public with whatever schema is of interest.","title":"What user owns tables in a schema"},{"location":"ssh/compare-private-and-public-ssh-key/","text":"Compare Private and Public SSH Key Pair diff <( ssh-keygen -y -e -f \"id_rsa\" ) <( ssh-keygen -y -e -f \"id_rsa.pub\" ) The above can be used to compare the ssh key pair used for ssh to ensure that the public key matches the private key.","title":"Compare Private and Public SSH Key Pair"},{"location":"ssh/compare-private-and-public-ssh-key/#compare-private-and-public-ssh-key-pair","text":"diff <( ssh-keygen -y -e -f \"id_rsa\" ) <( ssh-keygen -y -e -f \"id_rsa.pub\" ) The above can be used to compare the ssh key pair used for ssh to ensure that the public key matches the private key.","title":"Compare Private and Public SSH Key Pair"},{"location":"techblog/devto-api/","text":"DevTo API While the DevTo blog platform's API is still in Beta, there is a very clean and simple way of retrieving a JSON payload of a users Blog articles. curl https://dev.to/api/articles?username=jmoney8080","title":"DevTo API"},{"location":"techblog/devto-api/#devto-api","text":"While the DevTo blog platform's API is still in Beta, there is a very clean and simple way of retrieving a JSON payload of a users Blog articles. curl https://dev.to/api/articles?username=jmoney8080","title":"DevTo API"},{"location":"techblog/klipse/","text":"Klipse Klipse is a code snippet executor that can be used for showing and executing code snippets in tech blog examples.","title":"Klipse"},{"location":"techblog/klipse/#klipse","text":"Klipse is a code snippet executor that can be used for showing and executing code snippets in tech blog examples.","title":"Klipse"}]}