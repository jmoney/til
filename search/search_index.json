{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TIL A collection of knowledge I learn throughout the day. Do not want to write a full blown blog on the topics but do want to record them for other folks who stumble upon them or myself, I have a terrible memory.","title":"TIL"},{"location":"#til","text":"A collection of knowledge I learn throughout the day. Do not want to write a full blown blog on the topics but do want to record them for other folks who stumble upon them or myself, I have a terrible memory.","title":"TIL"},{"location":"ansible/facts/","text":"Facts Facts are a bit of data on the server that configuration frameworks like puppet and ansible can read and take advantage of. The facts for ansible go in /etc/ansible/facts.d and can be a file that contains JSON or INI format or an executable script that returns JSON. The extension is .fact . These facts are then available via ansible <hostname> -m setup /etc/ansible/facts.d/preferences.fact [general] asdf=1 bar=2 ansible <hostname> -m setup -a \"filter=ansible_local\" \"ansible_local\": { \"preferences\": { \"general\": { \"asdf\" : \"1\", \"bar\" : \"2\" } } } Reference Ansible Facts.d","title":"Facts"},{"location":"ansible/facts/#facts","text":"Facts are a bit of data on the server that configuration frameworks like puppet and ansible can read and take advantage of. The facts for ansible go in /etc/ansible/facts.d and can be a file that contains JSON or INI format or an executable script that returns JSON. The extension is .fact . These facts are then available via ansible <hostname> -m setup /etc/ansible/facts.d/preferences.fact [general] asdf=1 bar=2 ansible <hostname> -m setup -a \"filter=ansible_local\" \"ansible_local\": { \"preferences\": { \"general\": { \"asdf\" : \"1\", \"bar\" : \"2\" } } }","title":"Facts"},{"location":"ansible/facts/#reference","text":"Ansible Facts.d","title":"Reference"},{"location":"ansible/handlers/","text":"Handlers Sometimes you want to trigger other tasks only if certain tasks have changed. This is where the notify attribute and handlers come into play. tasks: - template: src: \"README.md.j2\" dest: \"README.md\" notify: commit and push handlers: - name: commit and push shell: | git config user.name \"CircleCI\" git config user.email \"noreply@circleci.com\" git add README.md git commit -m \"Added newly generated README\" git push origin master","title":"Handlers"},{"location":"ansible/handlers/#handlers","text":"Sometimes you want to trigger other tasks only if certain tasks have changed. This is where the notify attribute and handlers come into play. tasks: - template: src: \"README.md.j2\" dest: \"README.md\" notify: commit and push handlers: - name: commit and push shell: | git config user.name \"CircleCI\" git config user.email \"noreply@circleci.com\" git add README.md git commit -m \"Added newly generated README\" git push origin master","title":"Handlers"},{"location":"asciidoc/color-text/","text":"Color Text You can color text in asciidoc by doing [<color>] #text# where <color> is one of the sixteen HTML color names .","title":"Color Text"},{"location":"asciidoc/color-text/#color-text","text":"You can color text in asciidoc by doing [<color>] #text# where <color> is one of the sixteen HTML color names .","title":"Color Text"},{"location":"aws/dynamic-references-cfn/","text":"Dynamic References CFN Cloudformation can resolve properties from certain resources via dynamic reference syntax. Example is \"{{resolve:secretsmanager:MyRDSSecret:SecretString:username}}\"","title":"Dynamic References CFN"},{"location":"aws/dynamic-references-cfn/#dynamic-references-cfn","text":"Cloudformation can resolve properties from certain resources via dynamic reference syntax. Example is \"{{resolve:secretsmanager:MyRDSSecret:SecretString:username}}\"","title":"Dynamic References CFN"},{"location":"aws/ec2-instancetype-latest-generation/","text":"EC2 InstanceType Latest Generation Just by using the latest instancetype for an intance family can reduce the AWS bill. Example: use the m5 family over the m4 family. jmoney8080/ec2-instancetype-generation is a script that returns a JSON payload that describes which ec2 instanes are at latest and which are not.","title":"EC2 InstanceType Latest Generation"},{"location":"aws/ec2-instancetype-latest-generation/#ec2-instancetype-latest-generation","text":"Just by using the latest instancetype for an intance family can reduce the AWS bill. Example: use the m5 family over the m4 family. jmoney8080/ec2-instancetype-generation is a script that returns a JSON payload that describes which ec2 instanes are at latest and which are not.","title":"EC2 InstanceType Latest Generation"},{"location":"aws/lambda-container-image/","text":"Lambda Container Image AWS Lambda now supports container images. What this means is you can now deploy an AWS Lambda as a docker container. This seems to be a step forward from the \"bring your own runtimes\". new-for-aws-lambda-container-image-support","title":"Lambda Container Image"},{"location":"aws/lambda-container-image/#lambda-container-image","text":"AWS Lambda now supports container images. What this means is you can now deploy an AWS Lambda as a docker container. This seems to be a step forward from the \"bring your own runtimes\". new-for-aws-lambda-container-image-support","title":"Lambda Container Image"},{"location":"aws/lambda-docker-container/","text":"Lambda Docker Container docker run --rm -d \\ -e DOCKER_LAMBDA_STAY_OPEN=1 \\ -p 9001:9001 \\ -v $PWD:/var/task:ro,delegated \\ lambci/lambda:go1.x \\ main Run that in the code directory where the binary for the lambda you are executing exists. This will start up a local webserver in the same manner that AWS will run your lambda in. The lambda can now be invoked locally using aws lambda invoke --endpoint http://localhost:9001 --no-sign-request \\ --function-name myfunction --payload '{}' output.json or curl -d '{}' http://localhost:9001/2015-03-31/functions/myfunction/invocations The label of the docker image is lambda runtime that is to be used and the handler is what is set as the handler when createing a lambda. In the above example they are go1.x and main respectively.","title":"Lambda Docker Container"},{"location":"aws/lambda-docker-container/#lambda-docker-container","text":"docker run --rm -d \\ -e DOCKER_LAMBDA_STAY_OPEN=1 \\ -p 9001:9001 \\ -v $PWD:/var/task:ro,delegated \\ lambci/lambda:go1.x \\ main Run that in the code directory where the binary for the lambda you are executing exists. This will start up a local webserver in the same manner that AWS will run your lambda in. The lambda can now be invoked locally using aws lambda invoke --endpoint http://localhost:9001 --no-sign-request \\ --function-name myfunction --payload '{}' output.json or curl -d '{}' http://localhost:9001/2015-03-31/functions/myfunction/invocations The label of the docker image is lambda runtime that is to be used and the handler is what is set as the handler when createing a lambda. In the above example they are go1.x and main respectively.","title":"Lambda Docker Container"},{"location":"aws/list-all-instances-in-autoscaling-group/","text":"List All Instances in Autoscaling Group (export NAME=ASG_NAME; aws --profile dev-admin --region us-east-1 autoscaling describe-auto-scaling-groups --auto-scaling-group-names ${NAME} | jq -r '.AutoScalingGroups[].Instances[].InstanceId')","title":"List All Instances in Autoscaling Group"},{"location":"aws/list-all-instances-in-autoscaling-group/#list-all-instances-in-autoscaling-group","text":"(export NAME=ASG_NAME; aws --profile dev-admin --region us-east-1 autoscaling describe-auto-scaling-groups --auto-scaling-group-names ${NAME} | jq -r '.AutoScalingGroups[].Instances[].InstanceId')","title":"List All Instances in Autoscaling Group"},{"location":"aws/privatelink-alignment/","text":"PrivateLink Alignment It is a little known fact, but a rather annoying fact, that the availability zones for a PrivateLink must align between the provider and consumer. Example Provider is in availability zones: us-east-1a , us-east-1b , and us-east-1c Consumer is in availability zones: us-east-1b , us-east-1c , and us-east-1d The consumer IPs can ONLY be deployed to us-east-1b and us-east-1c as those are the AZs the provider is deployed too. This must be PHYSICAL availability zone. The availability zone us-east-1a is actually a virtual thing and maps to a physical availability zone. This mapping though can be different cross account. The describe-vpc-endpoints should tell you which availability zones you can deploy too as it handles the mapping into the account.","title":"PrivateLink Alignment"},{"location":"aws/privatelink-alignment/#privatelink-alignment","text":"It is a little known fact, but a rather annoying fact, that the availability zones for a PrivateLink must align between the provider and consumer.","title":"PrivateLink Alignment"},{"location":"aws/privatelink-alignment/#example","text":"Provider is in availability zones: us-east-1a , us-east-1b , and us-east-1c Consumer is in availability zones: us-east-1b , us-east-1c , and us-east-1d The consumer IPs can ONLY be deployed to us-east-1b and us-east-1c as those are the AZs the provider is deployed too. This must be PHYSICAL availability zone. The availability zone us-east-1a is actually a virtual thing and maps to a physical availability zone. This mapping though can be different cross account. The describe-vpc-endpoints should tell you which availability zones you can deploy too as it handles the mapping into the account.","title":"Example"},{"location":"aws/rds-replication-management/","text":"RDS Replication Management RDS provides an rds_replication role that is required for doing replication on RDS in postgres. This role does NOT have the REPLICATION priviledge required to muck with the WAL readers. So there must be some internal to RDS implementation that allows the replciation management. For instance, if you log into a postgres instance with a user with the rds_replication role you cannnot execute the pg_replication_slot_advance stored procedure to move the lsn for the slot around. Systems Administration The master user provided by RDS does not even have this role or SUPERUSER to grant this role. They have rds_superuser which does not have the ability to grant REPLICATION . Master User Accounts","title":"RDS Replication Management"},{"location":"aws/rds-replication-management/#rds-replication-management","text":"RDS provides an rds_replication role that is required for doing replication on RDS in postgres. This role does NOT have the REPLICATION priviledge required to muck with the WAL readers. So there must be some internal to RDS implementation that allows the replciation management. For instance, if you log into a postgres instance with a user with the rds_replication role you cannnot execute the pg_replication_slot_advance stored procedure to move the lsn for the slot around. Systems Administration The master user provided by RDS does not even have this role or SUPERUSER to grant this role. They have rds_superuser which does not have the ability to grant REPLICATION . Master User Accounts","title":"RDS Replication Management"},{"location":"aws/rds-update-configurations/","text":"RDS Update Configurations The user you get by default from RDS is an \"admin\" user but it is not THE admin user. To update configuration settings, like wal_sender_timeout , parameter groups must be used. If a query is executed in the console to update the configuration a permission denied will be thrown.","title":"RDS Update Configurations"},{"location":"aws/rds-update-configurations/#rds-update-configurations","text":"The user you get by default from RDS is an \"admin\" user but it is not THE admin user. To update configuration settings, like wal_sender_timeout , parameter groups must be used. If a query is executed in the console to update the configuration a permission denied will be thrown.","title":"RDS Update Configurations"},{"location":"aws/send-command-ssm/","text":"Send Command SSM Run command over instnces via multiple tags aws ssm send-command --document-name \"AWS-RunShellScript\" --parameters 'commands=[\"echo hello,world\"]' --targets Key=tag:tagA,Values=foo Key=tag:tagB,Values=Bar Key=tag:tabC,Values=Baz Notice the space between the \"Values\" and the \"Key\" in the --targets option.","title":"Send Command SSM"},{"location":"aws/send-command-ssm/#send-command-ssm","text":"","title":"Send Command SSM"},{"location":"aws/send-command-ssm/#run-command-over-instnces-via-multiple-tags","text":"aws ssm send-command --document-name \"AWS-RunShellScript\" --parameters 'commands=[\"echo hello,world\"]' --targets Key=tag:tagA,Values=foo Key=tag:tagB,Values=Bar Key=tag:tabC,Values=Baz Notice the space between the \"Values\" and the \"Key\" in the --targets option.","title":"Run command over instnces via multiple tags"},{"location":"aws/transit-gateway-cross-acount-sharing/","text":"Transit Gateway Cross Account Sharing In order to attach a VPC from Account 1 to a Transit Gateway in Account 2 the Transit Gateway in Acount 2 must be shared with Account 1 through resource sharing. This would require AWS Organizaions to be setup and enabled.","title":"Transit Gateway Cross Account Sharing"},{"location":"aws/transit-gateway-cross-acount-sharing/#transit-gateway-cross-account-sharing","text":"In order to attach a VPC from Account 1 to a Transit Gateway in Account 2 the Transit Gateway in Acount 2 must be shared with Account 1 through resource sharing. This would require AWS Organizaions to be setup and enabled.","title":"Transit Gateway Cross Account Sharing"},{"location":"aws/unused-ebs-volumes/","text":"Unused EBS Volumes Using the AWS cli it is possible to grab all the unused AWS volumes in an account/region pair. aws ec2 describe-volumes --filters Name=status,Values=available | jq . The above looks for all EBS volumes in the available state, but are not in-use or attached to an instance, and returns them all in JSON format. There are several states to look up. creating: Currently being created and initialized available: Available for use but not currently in use in-use: Currently being used or attached to an ec2 instance. deleting: Currently being deleted deleted: Has been deleted and AWS will reap shortly error: Some error has occured with the volume","title":"Unused EBS Volumes"},{"location":"aws/unused-ebs-volumes/#unused-ebs-volumes","text":"Using the AWS cli it is possible to grab all the unused AWS volumes in an account/region pair. aws ec2 describe-volumes --filters Name=status,Values=available | jq . The above looks for all EBS volumes in the available state, but are not in-use or attached to an instance, and returns them all in JSON format. There are several states to look up. creating: Currently being created and initialized available: Available for use but not currently in use in-use: Currently being used or attached to an ec2 instance. deleting: Currently being deleted deleted: Has been deleted and AWS will reap shortly error: Some error has occured with the volume","title":"Unused EBS Volumes"},{"location":"devto/devto-api/","text":"DevTo API While the DevTo blog platform's API is still in Beta, there is a very clean and simple way of retrieving a JSON payload of a users Blog articles. curl https://dev.to/api/articles?username=jmoney8080","title":"DevTo API"},{"location":"devto/devto-api/#devto-api","text":"While the DevTo blog platform's API is still in Beta, there is a very clean and simple way of retrieving a JSON payload of a users Blog articles. curl https://dev.to/api/articles?username=jmoney8080","title":"DevTo API"},{"location":"docker/docker-compose-down/","text":"Docker Compose Down Ever seen the error docker-compose up Recreating example-search-platform_datastore_1 ... error ERROR: for example-search-platform_datastore_1 b'no such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000: No such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000' ERROR: for datastore b'no such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000: No such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000' ERROR: The image for the service you're trying to recreate has been removed. If you continue, volume data could be lost. Consider backing up your data before continuing. That is because you did not run docker-compose down but instead ran something like docker rmi -f $(docker images -q) to clean out the image cache. Docker compose has a tempoarary image cache it uses for orchestration and when you blow away the main docker image cache docker compose will get very confused and try to recreate the image.","title":"Docker Compose Down"},{"location":"docker/docker-compose-down/#docker-compose-down","text":"Ever seen the error docker-compose up Recreating example-search-platform_datastore_1 ... error ERROR: for example-search-platform_datastore_1 b'no such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000: No such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000' ERROR: for datastore b'no such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000: No such image: sha256:b3def9a4744708e197d783b59f234a359116ef737af6600b5273c94cecdd7000' ERROR: The image for the service you're trying to recreate has been removed. If you continue, volume data could be lost. Consider backing up your data before continuing. That is because you did not run docker-compose down but instead ran something like docker rmi -f $(docker images -q) to clean out the image cache. Docker compose has a tempoarary image cache it uses for orchestration and when you blow away the main docker image cache docker compose will get very confused and try to recreate the image.","title":"Docker Compose Down"},{"location":"golang/goreleaser/","text":"goreleaser When releasing binaries built in golang the currect defacto release tool is goreleaser . Personally, I love to have homebrew versions of my tooling but creating and managing taps can be annoying. Luckily, gorelaser has a nice hook into doing that. Just add the following snippet. brews: - github: owner: jmoney8080 name: homebrew-aws folder: Formula homepage: https://www.jmoney8080.dev description: Collection of scripts that analyze easily correctable items to save money in AWS. test: | system \"#{bin}/aws-cost-maintenance -h\"","title":"goreleaser"},{"location":"golang/goreleaser/#goreleaser","text":"When releasing binaries built in golang the currect defacto release tool is goreleaser . Personally, I love to have homebrew versions of my tooling but creating and managing taps can be annoying. Luckily, gorelaser has a nice hook into doing that. Just add the following snippet. brews: - github: owner: jmoney8080 name: homebrew-aws folder: Formula homepage: https://www.jmoney8080.dev description: Collection of scripts that analyze easily correctable items to save money in AWS. test: | system \"#{bin}/aws-cost-maintenance -h\"","title":"goreleaser"},{"location":"haproxy/defer-server-dns-lookups/","text":"Defer Server DNS lookups By default, HAProxy on start up will resolve all server DNS entries before the process is ready for traffic. To turn off this behavior add init-addr none to you server line. backend my_backend server my_server www.example.com:80 init-addr none","title":"Defer Server DNS lookups"},{"location":"haproxy/defer-server-dns-lookups/#defer-server-dns-lookups","text":"By default, HAProxy on start up will resolve all server DNS entries before the process is ready for traffic. To turn off this behavior add init-addr none to you server line. backend my_backend server my_server www.example.com:80 init-addr none","title":"Defer Server DNS lookups"},{"location":"haproxy/failover/","text":"Failover HAProxy has failover! backend mybackend server s1 10.0.0.101:80 check server s2 10.0.0.102:80 check server s3 10.0.0.103:80 check backup server s4 10.0.0.104:80 check backup In the above config s1 and s2 are the primary servers. They have the check keyword turned on which turns on healthchecking. If both fail then s3 is pulled into rotation. If s3 fails then it will pull s4 into rotation. Finally, if s4 fails it will return a http status code 503 SERVICE UNAVAILABLE . Reference HAProxy Failover","title":"Failover"},{"location":"haproxy/failover/#failover","text":"HAProxy has failover! backend mybackend server s1 10.0.0.101:80 check server s2 10.0.0.102:80 check server s3 10.0.0.103:80 check backup server s4 10.0.0.104:80 check backup In the above config s1 and s2 are the primary servers. They have the check keyword turned on which turns on healthchecking. If both fail then s3 is pulled into rotation. If s3 fails then it will pull s4 into rotation. Finally, if s4 fails it will return a http status code 503 SERVICE UNAVAILABLE .","title":"Failover"},{"location":"haproxy/failover/#reference","text":"HAProxy Failover","title":"Reference"},{"location":"haproxy/haproxy-and-lua/","text":"HAProxy and Lua HAProxy can be extended to do a lot more than just simple load balancing using the lua scripting language. I could go talk about just a few items but it would not do it justice. This shows 5 different concepts in how you can extend HAProxy to do more with the simple lua scripting language.","title":"HAProxy and Lua"},{"location":"haproxy/haproxy-and-lua/#haproxy-and-lua","text":"HAProxy can be extended to do a lot more than just simple load balancing using the lua scripting language. I could go talk about just a few items but it would not do it justice. This shows 5 different concepts in how you can extend HAProxy to do more with the simple lua scripting language.","title":"HAProxy and Lua"},{"location":"haproxy/service-discovery/","text":"Service Discovery HAProxy can actually leverage DNS SRV records and build a service discovery setup using these SRV records. First we need to configure some resolver options resolvers mydns nameserver dns1 <name server> accepted_payload_size 8192 # allow larger DNS payloads The key here is the accept_payload_size . We need to increase this as SRV records can be a lot and do not fit in the default payload size. Next we need to configure what is called server-template s. Server templates are a server line under the backend that expands to the maximum entries. If you have server-template 5 it will expand to 5 backend server lines based on the number of SRV records returned. backend webservers balance roundrobin server-template web 5 myservice.example.local:80 check resolvers mydns init-addr none","title":"Service Discovery"},{"location":"haproxy/service-discovery/#service-discovery","text":"HAProxy can actually leverage DNS SRV records and build a service discovery setup using these SRV records. First we need to configure some resolver options resolvers mydns nameserver dns1 <name server> accepted_payload_size 8192 # allow larger DNS payloads The key here is the accept_payload_size . We need to increase this as SRV records can be a lot and do not fit in the default payload size. Next we need to configure what is called server-template s. Server templates are a server line under the backend that expands to the maximum entries. If you have server-template 5 it will expand to 5 backend server lines based on the number of SRV records returned. backend webservers balance roundrobin server-template web 5 myservice.example.local:80 check resolvers mydns init-addr none","title":"Service Discovery"},{"location":"postgres/active-wal-readers-on-slot/","text":"Active WAL Readers on Slot SELECT psa.usename, psa.application_name, psa.client_addr, psa.datname, prs.slot_name, psa.state, psa.wait_event, prs.restart_lsn, prs.confirmed_flush_lsn FROM pg_replication_slots prs JOIN pg_stat_activity psa ON prs.active_pid = psa.pid WHERE prs.slot_name = 'kinesis';","title":"Active WAL Readers on Slot"},{"location":"postgres/active-wal-readers-on-slot/#active-wal-readers-on-slot","text":"SELECT psa.usename, psa.application_name, psa.client_addr, psa.datname, prs.slot_name, psa.state, psa.wait_event, prs.restart_lsn, prs.confirmed_flush_lsn FROM pg_replication_slots prs JOIN pg_stat_activity psa ON prs.active_pid = psa.pid WHERE prs.slot_name = 'kinesis';","title":"Active WAL Readers on Slot"},{"location":"postgres/destructively-clear-out-replication-slot/","text":"Destructively Clear Out Replication Slot This is not to be done in a production environment. SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot eists SELECT pg_drop_replication_slot('slot_name'); -- Drop slot SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot does not exists SELECT pg_sleep(300); -- Wait for some specified amount of time for WAL files to clear SELECT pg_create_logical_replication_slot('slot_name', 'wal2json'); -- create new logical slot and hookup wal2json plugin SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot eists The above is useful for clearing out WAL files in RDS if the WAL reader fell behind and we just want to get the wal reader to the end of the WAL. Usually waiting 5 to 10 minutes is good enough to clear out all the obsolete WAL files. Running the query in Disk Space Queries should be a good gauge if the WAL files are deleted.","title":"Destructively Clear Out Replication Slot"},{"location":"postgres/destructively-clear-out-replication-slot/#destructively-clear-out-replication-slot","text":"This is not to be done in a production environment. SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot eists SELECT pg_drop_replication_slot('slot_name'); -- Drop slot SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot does not exists SELECT pg_sleep(300); -- Wait for some specified amount of time for WAL files to clear SELECT pg_create_logical_replication_slot('slot_name', 'wal2json'); -- create new logical slot and hookup wal2json plugin SELECT slot_name FROM pg_replication_slots WHERE slot_name = 'slot_name'; -- ensure slot eists The above is useful for clearing out WAL files in RDS if the WAL reader fell behind and we just want to get the wal reader to the end of the WAL. Usually waiting 5 to 10 minutes is good enough to clear out all the obsolete WAL files. Running the query in Disk Space Queries should be a good gauge if the WAL files are deleted.","title":"Destructively Clear Out Replication Slot"},{"location":"postgres/disk-space-queries/","text":"Disk Space Queries Here are a few queries to figure out what is consuming all the disk space in postgres WAL Files select count(*) \"#WAL files\", round(sum(size) / 1024.0/1024.0/1024.0,3) as \"Size (GB)\", min(modification) \"Oldest\", max(modification) \"Newest\" from pg_ls_waldir(); If the above query is showing substantial disk usage then run the following to determine what replication slot is behind select *, pg_current_wal_flush_lsn() from pg_replication_slots; Really Large Tables select nspname \"Schema\", relname \"Table\", reltuples \"#Rows Est\", pg_size_pretty(size) \"Disk Space\" from ( select n.nspname,c.relname,c.reltuples, pg_table_size(c.oid) size from pg_class c join pg_namespace n on n.oid = c.relnamespace where c.relkind = 'r' ) a order by size desc; The above will show how much disk is being consumed by tables.","title":"Disk Space Queries"},{"location":"postgres/disk-space-queries/#disk-space-queries","text":"Here are a few queries to figure out what is consuming all the disk space in postgres","title":"Disk Space Queries"},{"location":"postgres/disk-space-queries/#wal-files","text":"select count(*) \"#WAL files\", round(sum(size) / 1024.0/1024.0/1024.0,3) as \"Size (GB)\", min(modification) \"Oldest\", max(modification) \"Newest\" from pg_ls_waldir(); If the above query is showing substantial disk usage then run the following to determine what replication slot is behind select *, pg_current_wal_flush_lsn() from pg_replication_slots;","title":"WAL Files"},{"location":"postgres/disk-space-queries/#really-large-tables","text":"select nspname \"Schema\", relname \"Table\", reltuples \"#Rows Est\", pg_size_pretty(size) \"Disk Space\" from ( select n.nspname,c.relname,c.reltuples, pg_table_size(c.oid) size from pg_class c join pg_namespace n on n.oid = c.relnamespace where c.relkind = 'r' ) a order by size desc; The above will show how much disk is being consumed by tables.","title":"Really Large Tables"},{"location":"postgres/wal-senders-timeout/","text":"WAL Sender Timeout wal_sender_timeout (integer) Terminate replication connections that are inactive longer than the specified number of milliseconds. This is useful for the sending server to detect a standby crash or network outage. A value of zero disables the timeout mechanism. This parameter can only be set in the postgresql.conf file or on the server command line. The default value is 60 seconds. The following will show what the current value is for the wal_sender_timeout . SHOW wal_sender_timeout; The following will update the current value for wal_sender_timeout to 0 which disables the timeout. ALTER SYSTEM SET wal_sender_timeout TO 0;","title":"WAL Sender Timeout"},{"location":"postgres/wal-senders-timeout/#wal-sender-timeout","text":"wal_sender_timeout (integer) Terminate replication connections that are inactive longer than the specified number of milliseconds. This is useful for the sending server to detect a standby crash or network outage. A value of zero disables the timeout mechanism. This parameter can only be set in the postgresql.conf file or on the server command line. The default value is 60 seconds. The following will show what the current value is for the wal_sender_timeout . SHOW wal_sender_timeout; The following will update the current value for wal_sender_timeout to 0 which disables the timeout. ALTER SYSTEM SET wal_sender_timeout TO 0;","title":"WAL Sender Timeout"},{"location":"postgres/what-privileges-are-assigned-to-a-user/","text":"What privileges are assigned to a user SELECT grantee ,table_catalog ,table_schema ,table_name ,string_agg(privilege_type, ', ' ORDER BY privilege_type) AS privileges FROM information_schema.role_table_grants WHERE grantee != 'postgres' GROUP BY grantee, table_catalog, table_schema, table_name; The above query will list all users, who are not postgres, and their assigned privileges.","title":"What privileges are assigned to a user"},{"location":"postgres/what-privileges-are-assigned-to-a-user/#what-privileges-are-assigned-to-a-user","text":"SELECT grantee ,table_catalog ,table_schema ,table_name ,string_agg(privilege_type, ', ' ORDER BY privilege_type) AS privileges FROM information_schema.role_table_grants WHERE grantee != 'postgres' GROUP BY grantee, table_catalog, table_schema, table_name; The above query will list all users, who are not postgres, and their assigned privileges.","title":"What privileges are assigned to a user"},{"location":"postgres/what-user-owns-tables-in-a-schema/","text":"What user owns tables in a schema Ever wonder what users own tables in a schema? Well, this is how you do it in postgres select t.table_name, t.table_type, c.relname, c.relowner, u.usename from information_schema.tables t join pg_catalog.pg_class c on (t.table_name = c.relname) join pg_catalog.pg_user u on (c.relowner = u.usesysid) where t.table_schema='public'; In the query above replace public with whatever schema is of interest.","title":"What user owns tables in a schema"},{"location":"postgres/what-user-owns-tables-in-a-schema/#what-user-owns-tables-in-a-schema","text":"Ever wonder what users own tables in a schema? Well, this is how you do it in postgres select t.table_name, t.table_type, c.relname, c.relowner, u.usename from information_schema.tables t join pg_catalog.pg_class c on (t.table_name = c.relname) join pg_catalog.pg_user u on (c.relowner = u.usesysid) where t.table_schema='public'; In the query above replace public with whatever schema is of interest.","title":"What user owns tables in a schema"}]}